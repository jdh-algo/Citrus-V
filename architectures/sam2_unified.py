"""
ç»Ÿä¸€çš„SAM2æ¨¡å‹ï¼Œå…¼å®¹transformersæ ¼å¼
æ”¯æŒè®­ç»ƒå’Œæ¨ç†ä¸¤ç§æ¨¡å¼ï¼Œè‡ªåŠ¨æ ¹æ®é…ç½®é€‰æ‹©ç›¸åº”çš„å®ç°
"""

import os.path
from typing import Any, Dict, List, Optional, Tuple, Union

import torch
import torch.nn as nn
from architectures.vlm_utils import load_checkpoint_with_prefix, load_state_dict_to_model
from hydra import compose
from hydra.utils import instantiate
from omegaconf import OmegaConf
from transformers.configuration_utils import PretrainedConfig
from transformers.modeling_utils import PreTrainedModel


class SAM2UnifiedConfig(PretrainedConfig):
    """ç»Ÿä¸€çš„SAM2æ¨¡å‹é…ç½®ç±»"""

    model_type = 'sam2_unified'

    def __init__(
            self,
            model_path: str = '/mnt/workspace/offline/shared_models/sam2-hiera-large',
            cfg_path: str = 'sam2_hiera_l.yaml',
            mode: str = 'auto',  # "auto", "train", "inference"
            apply_postprocessing: bool = True,
            torch_dtype: str = 'bfloat16',
            hydra_overrides_extra: Optional[List[str]] = None,
            **kwargs):
        super().__init__(**kwargs)

        self.model_path = model_path
        self.cfg_path = cfg_path
        self.mode = mode
        self.apply_postprocessing = apply_postprocessing
        self.torch_dtype = torch_dtype
        self.hydra_overrides_extra = hydra_overrides_extra or []

        # è‡ªåŠ¨æ£€æµ‹æ¨¡å¼
        if self.mode == 'auto':
            # æ ¹æ®ç¯å¢ƒå˜é‡æˆ–é…ç½®è‡ªåŠ¨é€‰æ‹©æ¨¡å¼
            if os.environ.get('SAM2_MODE') == 'train':
                self.mode = 'train'
            elif os.environ.get('SAM2_MODE') == 'inference':
                self.mode = 'inference'
            else:
                # é»˜è®¤ä½¿ç”¨è®­ç»ƒæ¨¡å¼ï¼ˆæ›´å®‰å…¨ï¼‰
                self.mode = 'train'


class SAM2Unified(PreTrainedModel):
    """
    ç»Ÿä¸€çš„SAM2æ¨¡å‹ï¼Œå…¼å®¹transformersæ ¼å¼
    è‡ªåŠ¨æ ¹æ®é…ç½®é€‰æ‹©è®­ç»ƒæˆ–æ¨ç†æ¨¡å¼
    """

    config_class = SAM2UnifiedConfig
    base_model_prefix = 'sam2_unified'
    supports_gradient_checkpointing = True

    def __init__(self, config: SAM2UnifiedConfig):
        super().__init__(config)

        # è§£ææ•°æ®ç±»å‹
        if config.torch_dtype == 'float32':
            self.torch_dtype = torch.float32
        elif config.torch_dtype == 'float16':
            self.torch_dtype = torch.float16
        elif config.torch_dtype == 'bfloat16':
            self.torch_dtype = torch.bfloat16
        else:
            self.torch_dtype = torch.bfloat16

        # åˆå§‹åŒ–SAM2æ¨¡å‹
        self._init_sam2_model()

        # è®¾ç½®å›¾åƒé¢„å¤„ç†å‚æ•°
        self.img_mean = (0.485, 0.456, 0.406)
        self.img_std = (0.229, 0.224, 0.225)

        # è·å–éšè—ç»´åº¦
        self.hidden_dim = self.sam2_model.hidden_dim

        # è®¾ç½®æ¨¡å‹è·¯å¾„
        self.model_path = config.model_path
        self.ckpt_path = os.path.join(config.model_path, 'sam2_hiera_large.pt')

        # åˆå§‹åŒ–æƒé‡
        self.post_init()

    def _init_sam2_model(self):
        """åˆå§‹åŒ–SAM2æ¨¡å‹ï¼Œæ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„å®ç°"""
        from .third_parts import sam2  # noqa: F401

        # æ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„ç›®æ ‡ç±»
        if self.config.mode == 'train':
            target_class = 'architectures.sam2_base.SAM2Base'
        elif self.config.mode == 'inference':
            target_class = 'architectures.sam2_predictor.SAM2VideoPredictor'
        else:
            raise ValueError(f'Unknown mode: {self.config.mode}')

        # æ„å»ºhydraé…ç½®
        hydra_overrides = [
            f'++model._target_={target_class}',
        ]

        if self.config.apply_postprocessing:
            postprocessing_overrides = [
                # åŠ¨æ€å¤šæ©ç å›é€€
                '++model.sam_mask_decoder_extra_args.dynamic_multimask_via_stability=true',
                '++model.sam_mask_decoder_extra_args.dynamic_multimask_stability_delta=0.05',
                '++model.sam_mask_decoder_extra_args.dynamic_multimask_stability_thresh=0.98',
                # å…¶ä»–åå¤„ç†é€‰é¡¹å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ 
            ]
            hydra_overrides.extend(postprocessing_overrides)

        # æ·»åŠ é¢å¤–çš„è¦†ç›–
        if self.config.hydra_overrides_extra:
            hydra_overrides.extend(self.config.hydra_overrides_extra)

        # è§£æé…ç½®
        if os.path.isabs(self.config.cfg_path):
            config_dir = os.path.dirname(self.config.cfg_path)
            config_name = os.path.basename(self.config.cfg_path).replace('.yaml', '')
            cfg = compose(config_name=config_name, overrides=hydra_overrides, config_dir=config_dir)
        else:
            cfg = compose(config_name=self.config.cfg_path, overrides=hydra_overrides)

        OmegaConf.resolve(cfg)

        # å®ä¾‹åŒ–æ¨¡å‹
        self.sam2_model = instantiate(cfg.model, _recursive_=True)

        print(f'âœ… SAM2æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å¼: {self.config.mode}')

    def post_init(self):
        """ååˆå§‹åŒ–ï¼ŒåŠ è½½é¢„è®­ç»ƒæƒé‡"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ æƒé‡åŠ è½½é€»è¾‘
        pass

    def load_ori_state_dict(self):
        """åŠ è½½åŸå§‹çŠ¶æ€å­—å…¸"""
        state_dict = load_checkpoint_with_prefix(self.ckpt_path)
        load_state_dict_to_model(self.sam2_model, state_dict)
        print(f'âœ… åŸå§‹æƒé‡åŠ è½½æˆåŠŸ: {self.ckpt_path}')

    def preprocess_image(self, image: torch.Tensor) -> torch.Tensor:
        """å›¾åƒé¢„å¤„ç†"""
        image = image / 255.
        img_mean = torch.tensor(self.img_mean, dtype=image.dtype, device=image.device)[:, None, None]
        img_std = torch.tensor(self.img_std, dtype=image.dtype, device=image.device)[:, None, None]
        image -= img_mean
        image /= img_std
        return image

    def get_sam2_embeddings(self, images: torch.Tensor, expand_size: int = 1) -> Dict[str, Any]:
        """è·å–SAM2åµŒå…¥ç‰¹å¾"""
        images = images.to(dtype=self.torch_dtype)

        if self.config.mode == 'train':
            # è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨forward_image
            with torch.autocast(device_type='cuda', dtype=self.torch_dtype):
                feats = self.sam2_model.forward_image(images)

            if expand_size > 1:
                # æ‰©å±•ç‰¹å¾ç»´åº¦
                for i, feat in enumerate(feats['backbone_fpn']):
                    feats['backbone_fpn'][i] = feat[:, None].expand(-1, expand_size, -1, -1,
                                                                    -1).flatten(0, 1).contiguous()
                for i, pos in enumerate(feats['vision_pos_enc']):
                    pos = pos[:, None].expand(-1, expand_size, -1, -1, -1).flatten(0, 1).contiguous()
                    feats['vision_pos_enc'][i] = pos

            # å‡†å¤‡ç‰¹å¾
            _, current_vision_feats, current_vision_pos_embeds, feat_sizes = self.sam2_model._prepare_backbone_features(
                feats)

            return {
                'current_vision_feats': current_vision_feats,
                'current_vision_pos_embeds': current_vision_pos_embeds,
                'feat_sizes': feat_sizes,
            }

        else:
            # æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨init_state
            return self.sam2_model.init_state(images)

    def inject_language_embd(self, sam_states: Dict[str, Any], language_embd: torch.Tensor) -> torch.Tensor:
        """æ³¨å…¥è¯­è¨€åµŒå…¥ï¼Œç”Ÿæˆåˆ†å‰²æ©ç """
        if self.config.mode == 'train':
            # è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨_forward_sam_heads
            return self._inject_language_embd_train(sam_states, language_embd)
        else:
            # æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨add_language_embd
            return self._inject_language_embd_inference(sam_states, language_embd)

    def _inject_language_embd_train(self, sam_states: Dict[str, Any], language_embd: torch.Tensor) -> torch.Tensor:
        """è®­ç»ƒæ¨¡å¼çš„è¯­è¨€åµŒå…¥æ³¨å…¥"""
        high_res_features = [
            x.permute(1, 2, 0).view(x.size(1), x.size(2), *s).contiguous()
            for x, s in zip(sam_states['current_vision_feats'][:-1], sam_states['feat_sizes'][:-1])
        ]

        B = sam_states['current_vision_feats'][-1].size(1)
        C = self.hidden_dim
        H, W = sam_states['feat_sizes'][-1]

        if self.sam2_model.directly_add_no_mem_embed:
            # ç›´æ¥æ·»åŠ æ— è®°å¿†åµŒå…¥
            pix_feat = sam_states['current_vision_feats'][-1]
            no_mem_embed = self.sam2_model.no_mem_embed.to(pix_feat.device)
            pix_feat_with_mem = pix_feat + no_mem_embed
            pix_feat_with_mem = pix_feat_with_mem.permute(1, 2, 0).view(B, C, H, W).contiguous()
        else:
            raise NotImplementedError('directly add no memory embedding is not implemented')

        with torch.autocast(device_type='cuda', dtype=self.torch_dtype):
            _, _, _, low_res_masks, high_res_masks, obj_ptr, _, = self.sam2_model._forward_sam_heads(
                backbone_features=pix_feat_with_mem,
                point_inputs=None,
                mask_inputs=None,
                high_res_features=high_res_features,
                multimask_output=self.sam2_model._use_multimask(is_init_cond_frame=True, point_inputs=None),
                language_embd=language_embd,
            )

        return low_res_masks

    def _inject_language_embd_inference(self, inference_state: Any,
                                        language_embd: List[List[torch.Tensor]]) -> torch.Tensor:
        """æ¨ç†æ¨¡å¼çš„è¯­è¨€åµŒå…¥æ³¨å…¥"""
        num_frame = len(language_embd)
        num_obj = len(language_embd[0])
        mask_out = []

        for frame_idx in range(num_frame):
            frame_mask_out = []
            for obj_idx in range(num_obj):
                _language_embd = language_embd[frame_idx][obj_idx][None][None]
                _, _, out_mask_logits = self.sam2_model.add_language_embd(inference_state, frame_idx, obj_idx + 100,
                                                                          _language_embd)
                frame_mask_out.append(out_mask_logits)
            frame_mask_out = torch.cat(frame_mask_out, dim=1)
            mask_out.append(frame_mask_out)

        mask_out = torch.cat(mask_out, dim=0)
        return mask_out

    def language_embd_inference(self, inference_state: Any, language_embd: List[List[torch.Tensor]]) -> torch.Tensor:
        """æ¨ç†æ¨¡å¼çš„è¯­è¨€åµŒå…¥æ¨ç†ï¼ˆè§†é¢‘ä¼ æ’­ï¼‰"""
        if self.config.mode != 'inference':
            raise ValueError('language_embd_inference only available in inference mode')

        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
            mask_out = []
            for out_frame_idx, out_obj_ids, out_mask_logits in self.sam2_model.propagate_in_video(inference_state):
                mask_out.append(out_mask_logits)
            mask_out = torch.cat(mask_out, dim=0)

        return mask_out

    def forward(self, batch: Any) -> Any:
        """å‰å‘ä¼ æ’­ï¼ˆéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å®ç°ï¼‰"""
        raise NotImplementedError('forward method needs to be implemented for specific tasks')

    def get_trainable_parameters(self) -> List[nn.Parameter]:
        """è·å–å¯è®­ç»ƒå‚æ•°"""
        return list(self.parameters())

    def get_frozen_parameters(self) -> List[nn.Parameter]:
        """è·å–å†»ç»“å‚æ•°"""
        return []

    def set_mode(self, mode: str):
        """åŠ¨æ€è®¾ç½®æ¨¡å¼"""
        if mode not in ['train', 'inference']:
            raise ValueError(f'Invalid mode: {mode}')

        if mode != self.config.mode:
            print(f'âš ï¸ è­¦å‘Šï¼šåŠ¨æ€åˆ‡æ¢æ¨¡å¼ä» {self.config.mode} åˆ° {mode} å¯èƒ½ä¸å®‰å…¨')
            # è¿™é‡Œå¯ä»¥å®ç°åŠ¨æ€æ¨¡å¼åˆ‡æ¢çš„é€»è¾‘
            pass

    def save_pretrained(self, save_directory: str, **kwargs):
        """ä¿å­˜é¢„è®­ç»ƒæ¨¡å‹"""
        super().save_pretrained(save_directory, **kwargs)

        # ä¿å­˜SAM2ç‰¹å®šé…ç½®
        sam2_config = {
            'model_path': self.model_path,
            'cfg_path': self.config.cfg_path,
            'mode': self.config.mode,
            'apply_postprocessing': self.config.apply_postprocessing,
            'torch_dtype': self.config.torch_dtype,
        }

        config_path = os.path.join(save_directory, 'sam2_config.json')
        import json
        with open(config_path, 'w') as f:
            json.dump(sam2_config, f, indent=2)

        print(f'âœ… SAM2é…ç½®å·²ä¿å­˜åˆ°: {config_path}')

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path: str, *model_args, **kwargs):
        """ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½"""
        # é¦–å…ˆè°ƒç”¨çˆ¶ç±»æ–¹æ³•
        model = super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)

        # å°è¯•åŠ è½½SAM2ç‰¹å®šé…ç½®
        sam2_config_path = os.path.join(pretrained_model_name_or_path, 'sam2_config.json')
        if os.path.exists(sam2_config_path):
            import json
            with open(sam2_config_path, 'r') as f:
                sam2_config = json.load(f)

            # æ›´æ–°é…ç½®
            for key, value in sam2_config.items():
                if hasattr(model.config, key):
                    setattr(model.config, key, value)

            print(f'âœ… SAM2é…ç½®å·²ä» {sam2_config_path} åŠ è½½')

        return model


# å…¼å®¹æ€§åˆ«å
SAM2TrainRunner = SAM2Unified
SAM2 = SAM2Unified


def create_sam2_model(mode: str = 'auto',
                      model_path: str = '/mnt/workspace/offline/shared_models/sam2-hiera-large',
                      cfg_path: str = 'sam2_hiera_l.yaml',
                      apply_postprocessing: bool = True,
                      torch_dtype: str = 'bfloat16',
                      **kwargs) -> SAM2Unified:
    """åˆ›å»ºSAM2æ¨¡å‹çš„ä¾¿æ·å‡½æ•°"""

    config = SAM2UnifiedConfig(
        model_path=model_path,
        cfg_path=cfg_path,
        mode=mode,
        apply_postprocessing=apply_postprocessing,
        torch_dtype=torch_dtype,
        **kwargs)

    return SAM2Unified(config)


# æµ‹è¯•å‡½æ•°
def test_sam2_unified():
    """æµ‹è¯•ç»Ÿä¸€çš„SAM2æ¨¡å‹"""
    print('=== æµ‹è¯•ç»Ÿä¸€çš„SAM2æ¨¡å‹ ===')

    # æµ‹è¯•1ï¼šè®­ç»ƒæ¨¡å¼
    print('\n--- æµ‹è¯•è®­ç»ƒæ¨¡å¼ ---')
    try:
        train_model = create_sam2_model(mode='train', cfg_path='sam2_hiera_l.yaml', apply_postprocessing=False)
        print('âœ… è®­ç»ƒæ¨¡å¼æ¨¡å‹åˆ›å»ºæˆåŠŸ')
        print(f'  æ¨¡å¼: {train_model.config.mode}')
        print(f'  éšè—ç»´åº¦: {train_model.hidden_dim}')
        print(f'  æ•°æ®ç±»å‹: {train_model.torch_dtype}')
    except Exception as e:
        print(f'âŒ è®­ç»ƒæ¨¡å¼æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}')
        import traceback
        traceback.print_exc()

    # æµ‹è¯•2ï¼šæ¨ç†æ¨¡å¼
    print('\n--- æµ‹è¯•æ¨ç†æ¨¡å¼ ---')
    try:
        inference_model = create_sam2_model(mode='inference', cfg_path='sam2_hiera_l.yaml', apply_postprocessing=True)
        print('âœ… æ¨ç†æ¨¡å¼æ¨¡å‹åˆ›å»ºæˆåŠŸ')
        print(f'  æ¨¡å¼: {inference_model.config.mode}')
        print(f'  éšè—ç»´åº¦: {inference_model.hidden_dim}')
        print(f'  æ•°æ®ç±»å‹: {inference_model.torch_dtype}')
    except Exception as e:
        print(f'âŒ æ¨ç†æ¨¡å¼æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}')
        import traceback
        traceback.print_exc()

    # æµ‹è¯•3ï¼šè‡ªåŠ¨æ¨¡å¼
    print('\n--- æµ‹è¯•è‡ªåŠ¨æ¨¡å¼ ---')
    try:
        auto_model = create_sam2_model(mode='auto', cfg_path='sam2_hiera_l.yaml', apply_postprocessing=False)
        print('âœ… è‡ªåŠ¨æ¨¡å¼æ¨¡å‹åˆ›å»ºæˆåŠŸ')
        print(f'  æ¨¡å¼: {auto_model.config.mode}')
        print(f'  éšè—ç»´åº¦: {auto_model.hidden_dim}')
        print(f'  æ•°æ®ç±»å‹: {auto_model.torch_dtype}')
    except Exception as e:
        print(f'âŒ è‡ªåŠ¨æ¨¡å¼æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}')
        import traceback
        traceback.print_exc()

    print('\nğŸ‰ æµ‹è¯•å®Œæˆï¼')


if __name__ == '__main__':
    test_sam2_unified()
